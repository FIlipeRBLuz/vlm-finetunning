{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor, VisionEncoderDecoderModel, AutoProcessor, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "import pdf2image\n",
    "import win32com.client\n",
    "import win32com.client.connect\n",
    "#from pptxtopdf import convert\n",
    "import torch\n",
    "#poppler library path\n",
    "os.environ[\"PATH\"] = r\"C:\\Users\\filip\\OneDrive\\Área de Trabalho\\Cast Group\\Projects\\for_all_see_it_cast\\poppler-24.07.0\\Library\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import win32com.client\n",
    "import pandas as pd\n",
    "\n",
    "def extrair_slides_e_comentarios(diretorio):\n",
    "    # Inicializa o PowerPoint\n",
    "    ppt_app = win32com.client.Dispatch(\"PowerPoint.Application\")\n",
    "    ppt_app.Visible = True\n",
    "        \n",
    "    # Lista para armazenar os dados\n",
    "    dados = []\n",
    "\n",
    "    # Itera sobre todos os arquivos no diretório\n",
    "    for arquivo in os.listdir(diretorio):\n",
    "        if arquivo.endswith('.pptx'):\n",
    "            pptx_path = os.path.join(diretorio, arquivo)\n",
    "            apresentacao = ppt_app.Presentations.Open(pptx_path)\n",
    "\n",
    "            # Processa cada slide\n",
    "            for slide_index, ppt_slide in enumerate(apresentacao.Slides):\n",
    "                # Define o caminho da imagem\n",
    "                imagem_path = os.path.join(diretorio, f\"{arquivo[:-5]}_slide_{slide_index + 1}.png\")\n",
    "                \n",
    "                # Salva o slide como imagem\n",
    "                ppt_slide.Export(imagem_path, 'PNG')\n",
    "                \n",
    "                # Extrai comentários\n",
    "                comentarios = [comment.Text for comment in ppt_slide.Comments]\n",
    "\n",
    "                # Adiciona os dados ao DataFrame\n",
    "                for comentario in comentarios:\n",
    "                    dados.append({\n",
    "                        'nome_arquivo': arquivo,\n",
    "                        'image': imagem_path.replace('\\\\','/'),\n",
    "                        'text': comentario.replace('\\n\\n',' ').replace('\\n',' ').strip()\n",
    "                    })\n",
    "\n",
    "            # Fecha a apresentação\n",
    "            apresentacao.Close()\n",
    "\n",
    "    # Cria um DataFrame a partir dos dados\n",
    "    df = pd.DataFrame(dados)\n",
    "\n",
    "    # Encerra o PowerPoint\n",
    "    ppt_app.Quit()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Exemplo de uso\n",
    "diretorio_pptx = \"C:\\\\Users\\\\filip\\\\OneDrive\\\\Área de Trabalho\\\\Cast Group\\\\Projects\\\\for_all_see_it_cast\\\\data\\\\pptx\\\\\"\n",
    "df_resultado = extrair_slides_e_comentarios(diretorio_pptx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome_arquivo</th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: A imagem tem um fundo pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é divi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é divi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            nome_arquivo  \\\n",
       "0  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "1  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "2  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "3  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "4  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "\n",
       "                                               image  \\\n",
       "0  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "1  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "2  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "3  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "4  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "\n",
       "                                                text  \n",
       "0  #PraTodosVerem Fundo: A imagem tem um fundo pr...  \n",
       "1  #PraTodosVerem Fundo: O fundo da imagem é bran...  \n",
       "2  #PraTodosVerem Fundo: O fundo da imagem é bran...  \n",
       "3  #PraTodosVerem Fundo: O fundo da imagem é divi...  \n",
       "4  #PraTodosVerem Fundo: O fundo da imagem é divi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.46.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loc = \"ydshieh/vit-gpt2-coco-en\"\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(loc)\n",
    "processor = AutoProcessor.from_pretrained(loc)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"ybelkada/football-dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome_arquivo</th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: A imagem tem um fundo pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é divi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decima_Jornada_da_Agilidade_Cast.pptx</td>\n",
       "      <td>C:/Users/filip/OneDrive/Área de Trabalho/Cast ...</td>\n",
       "      <td>#PraTodosVerem Fundo: O fundo da imagem é divi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            nome_arquivo  \\\n",
       "0  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "1  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "2  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "3  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "4  Decima_Jornada_da_Agilidade_Cast.pptx   \n",
       "\n",
       "                                               image  \\\n",
       "0  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "1  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "2  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "3  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "4  C:/Users/filip/OneDrive/Área de Trabalho/Cast ...   \n",
       "\n",
       "                                                text  \n",
       "0  #PraTodosVerem Fundo: A imagem tem um fundo pr...  \n",
       "1  #PraTodosVerem Fundo: O fundo da imagem é bran...  \n",
       "2  #PraTodosVerem Fundo: O fundo da imagem é bran...  \n",
       "3  #PraTodosVerem Fundo: O fundo da imagem é divi...  \n",
       "4  #PraTodosVerem Fundo: O fundo da imagem é divi...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset\n",
    "df_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_read(image_path):\n",
    "        return Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado['image_rgb'] = df_resultado.image.apply(image_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 1]\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = self.data.iloc[idx, 1]\n",
    "\n",
    "        return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageTextDataset(df_resultado[['image_rgb','text']], transform=transforms.ToTensor())\n",
    "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, feature_extractor):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        img_path = item['image']\n",
    "        text = item['text']\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        encoding = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"text\"] = text\n",
    "\n",
    "        return encoding\n",
    "\n",
    "# Example usage with your DataFrame 'df_resultado'\n",
    "train_dataset = ImageCaptioningDataset(df_resultado, processor, feature_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lora configuration\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\",\"value\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "## Trainable Params\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params, all_param = 0, 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad: trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable %: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589824 || all params: 239785728 || trainable %: 0.2459796105963404\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    processed_batch = {}\n",
    "    for key in batch[0]:\n",
    "        if key != \"text\":\n",
    "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "        else:\n",
    "            text_inputs = processor([example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\")\n",
    "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "    return processed_batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=3, collate_fn=collator)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 done!, Loss: 5.834036827087402\n",
      "Epoch 20 done!, Loss: 5.328901767730713\n",
      "Epoch 30 done!, Loss: 5.839601516723633\n",
      "Epoch 40 done!, Loss: 5.753589153289795\n",
      "Epoch 50 done!, Loss: 5.513179302215576\n",
      "Epoch 60 done!, Loss: 5.513029098510742\n",
      "Epoch 70 done!, Loss: 5.155308246612549\n",
      "Epoch 80 done!, Loss: 5.544610500335693\n",
      "Epoch 90 done!, Loss: 5.214959621429443\n",
      "Epoch 100 done!, Loss: 5.739976406097412\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "model.train()\n",
    "for epoch in range(1, epochs+1):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0: print(f\"Epoch {epoch} done!, Loss: {loss.item()}\")\n",
    "    loss_list.append(loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(loc)\n",
    "\n",
    "def predict(image):\n",
    "\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True,\n",
    "  pad_token_id=tokenizer.eos_token_id).sequences\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Image.open('C:\\\\Users\\\\filip\\\\OneDrive\\\\Imagens\\\\vinijr.png') as image:\n",
    "#     preds = predict(image)\n",
    "image = Image.open(\"C:\\\\Users\\\\filip\\\\OneDrive\\\\Área de Trabalho\\\\Cast Group\\\\Projects\\\\for_all_see_it_cast\\\\data\\\\pptx\\\\Jornada da Agilidade Cast - com PTV 1_slide_2.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a black and white photo of a black and white photo of a black and']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
